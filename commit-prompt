#!/usr/bin/env bash

#  Copyright 2025 Stanislav Senotrusov
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.

# Define an array of ordered pairs representing model types and their names.
#
# Format:
#   ("model_type" "model_name")
#
# Supported model types:
#
# * gemini:
#   - Sends API calls directly to Gemini using `curl`.
#   - Available model names can be found at:
#     https://ai.google.dev/gemini-api/docs/models
#
# * aichat:
#   - Command-line interface for language models.
#   - Specify a model name or use "--" for the default model.
#   - More details at:
#     https://github.com/sigoden/aichat
#
# * llm:
#   - Command-line interface for language models.
#   - Specify a model name or use "--" for the default model.
#   - More details at:
#     https://github.com/simonw/llm
#
# * ollama:
#   - Command-line interface for language models.
#   - A model name must be specified.
#   - More details at:
#     https://github.com/ollama/ollama
#
# * sgpt:
#   - Command-line interface for language models.
#   - Specify a model name or use "--" for the default model.
#   - More details at:
#     https://github.com/tbckr/sgpt
#
models=(
  # aichat --
  # gemini gemini-2.0-flash-lite
  gemini gemini-2.5-pro-exp-03-25
  gemini gemini-2.5-flash-preview-04-17
  gemini gemini-2.0-flash
)

# Define the maximum character limit for the ChatGPT web interface.
chatgpt_limit=139000

# ## `is_any_argument`
#
# Checks if a specific argument is present in a list of provided arguments.
#
# This function iterates over a list of arguments starting from the second
# parameter and checks if any of them match the first parameter.
#
# ### Usage
#
# is_any_argument <argument-to-find> [<arguments-to-search-in>...]
#
# Arguments:
#   <argument-to-find>        The argument to search for in the list
#   [<arguments-to-search-in>...] A space-separated list of arguments to search within
#
# ### Examples
#
# is_any_argument "--help" "--help" "-v" "--config"
# is_any_argument "test" "run" "build" "test"
#
is_any_argument() {
  local arg
  # Iterate over arguments starting from the second one
  for arg in "${@:2}"; do
    # Check if the current argument matches the search target
    [ "${arg}" = "$1" ] && return 0
  done
  # If no match is found after checking all arguments, return 1 (false)
  return 1
}

# ## `copy_to_clipboard`
#
# Copies standard input to the system clipboard.
#
# This function attempts to use various clipboard tools (wl-copy, xclip,
# pbcopy) in a preferred order. If none of these tools are available,
# it prints the input to standard output as a fallback.
#
# ### Usage
#
# copy_to_clipboard
#
copy_to_clipboard() {
  # Attempt to use wl-copy if available
  if command -v wl-copy >/dev/null 2>&1; then
    wl-copy || { echo "Could not access the clipboard using wl-copy." >&2; return 1; }

  # Else, attempt to use xclip if available
  elif command -v xclip >/dev/null 2>&1; then
    xclip -selection clipboard || { echo "Could not access the clipboard using xclip." >&2; return 1; }

  # Else, attempt to use pbcopy if available
  elif command -v pbcopy >/dev/null 2>&1; then
    pbcopy || { echo "Could not access the clipboard using pbcopy." >&2; return 1; }
  
  # If no clipboard tool is found, print to standard output
  else
    echo "No compatible clipboard tool found. Printing to standard output instead:" >&2
    cat || { echo "Could not read from standard input using cat." >&2; return 1; }
    return # Indicates that data was output via cat, not necessarily an error
  fi

  # Confirm that the prompt was successfully copied to the clipboard
  echo "Prompt copied to the clipboard." >&2
}

# ## `query_language_model`
#
# Send a prompt to the language model API and retrieves the plain-text response.
#
# This function reads a prompt from `stdin`, wraps it in a JSON payload, calls the specified
# model with `curl`, then pipes the result through `jq` to pull out the generated text.
#
# ### Usage
#
# query_language_model <model-type> <model-name>
#
# Arguments:
#   <model-type>  The type of the model, e.g. `gemini`
#   <model-name>  The specific model identifier, e.g. `gemini-2.0-flash`
#
# ### Examples
#
# echo "Explain quantum computing in simple terms." | query_language_model gemini gemini-2.0-flash
#
query_language_model() {
  # Assign the model type and name provided as arguments
  local model_type="$1"
  local model_name="$2"

  # Declare local variables
  local command_array
  local input_text_json
  local model_name_human
  local request_body
  local request_body_template
  local request_url
  local response_body
  local response_error_filter
  local response_filter

  # Determine the API endpoint based on the model type
  case "${model_type}" in
    gemini)
      # Check if the Gemini API key is available as an environment variable
      if [ -z "${GEMINI_API_KEY:-}" ]; then
        echo "The GEMINI_API_KEY environment variable is not set. Unable to query the model." >&2
        return 1
      fi

      # Construct the API URL for the request using the model name and API key
      request_url="https://generativelanguage.googleapis.com/v1beta/models/${model_name}:generateContent?key=${GEMINI_API_KEY}"
      
      # Create the JSON-formatted request body template with the placeholder for input text
      request_body_template='{"contents": [{"role": "user", "parts":[{"text": %s}]}]}'

      # Define the jq filter to extract the generated text from the API response
      response_filter='.candidates[].content.parts[].text'
      
      # Define the jq filter to extract the error message if the API request fails
      response_error_filter='.error.message'
      ;;
    aichat|llm|ollama|sgpt)
      # Initialize the human-readable model name with the original model name value
      model_name_human="${model_name}"

      # Populate the command array based on the specified model type
      case "${model_type}" in
        aichat|llm|sgpt)
          # For 'aichat', 'llm', and 'sgpt', the command is simply the model type itself
          command_array=("${model_type}")

          # Check if a specific model name is provided
          if [ "${model_name}" != "--" ]; then
            # Append the model name to the command array if it is specified (not "--")
            command_array+=(--model "${model_name}")
          else
            # Assign "default" as a user-friendly label if the model name is not provided
            model_name_human=default
          fi
        ;;
        ollama)
          # For 'ollama', the command is "ollama run" followed by the model name
          command_array=(ollama run "${model_name}")
        ;;
      esac

      # Display information about the request being sent to the specified model and command
      echo "Sending request to '${model_type}' command with language model '${model_name_human}'..." >&2

      # Run the command with the constructed arguments and handle any errors if it fails
      "${command_array[@]}" || {
        echo "Failed to run '${model_type}' with model '${model_name_human}'." >&2
        return 1
      }

      # Return the status code of the previous command to propagate its outcome
      return
      ;;
    *)
      echo "Unsupported model type '${model_type}'." >&2
      return 1
      ;;
  esac

  # Prepare the input text by converting it to a JSON string value using jq
  # This ensures special characters are correctly escaped for the JSON payload
  input_text_json=$(jq --ascii-output --raw-input --slurp .) ||
    { echo "Failed to convert input text to JSON format using 'jq'." >&2; return 1; }

  # Build the request body from the template
  # shellcheck disable=SC2059
  printf -v request_body "${request_body_template}" "${input_text_json}" ||
    { echo "Failed to construct the request body from the template." >&2; return 1; }

  # Inform the user which model is being queried
  echo "Sending request to language model '${model_name}' (${model_type})..." >&2

  # Send the HTTP POST request to the language model API using curl
  # Capture the output and check the exit status of the curl command
  if response_body="$(curl "${request_url}" \
    --request POST \
    --header 'Content-Type: application/json' \
    --data @- <<<"${request_body}" \
    --fail-with-body \
    --show-error \
    --silent)"
  then
    # Successfully received a response from the API

    # Extract the plain text content from the candidates array in the JSON response
    <<<"${response_body}" jq --raw-output "${response_filter}" ||
      { echo "Failed to extract content from the response for model '${model_name}' (${model_type})." >&2; return 1; }
  else
    # An error occurred during the API request
    echo "Could not retrieve a response from the language model '${model_name}' (${model_type})." >&2

    # Attempt to extract and display the specific error message from the API response
    <<<"${response_body}" jq --raw-output "${response_error_filter}" >&2 ||
      { echo "Failed to parse the error response from language model '${model_name}' (${model_type})." >&2; return 1; }

    # Return a non-zero status code to indicate failure
    return 1
  fi
}

# ## `make_prompt`
#
# Generates a structured prompt for a commit message based on Git changes.
#
# This function creates a detailed prompt by analyzing staged Git changes
# (diff) and recent commit history. If the "short" keyword is provided as an
# argument, it generates a more concise prompt using a minimal diff.
#
# ### Usage
#
# make_prompt [<keyword>]
#
# Arguments:
#   [<keyword>]  Optional keyword. If "short", generates a prompt with
#                minimal diff context. Otherwise, a more detailed diff
#                and recent commit history are included.
#
# ### Examples
#
# make_prompt
# make_prompt short
#
make_prompt() {
  local context_messages=12

  # If the "short" keyword is provided, reduce the number of context messages
  if is_any_argument short "$@"; then
    context_messages=4
  fi

  # Begin constructing the prompt with formatting instructions and guidelines
  cat <<EOF
Based on the changes described in the diff below, write a commit message.

* Follow these formatting rules:
  * The first line must be a concise summary of the entire commit
    * Limit: 50 characters maximum
    * Do not prefix it with '#'
  * The commit body must follow the summary
    * Format: Markdown using '*' as the bullet symbol
    * Do not use bold, italic, or other text formatting
    * Do not include any trailing whitespace
    * Limit: 72 characters per line
  * Commit message should start with a capital letter

* In writing commit message ensure:
  * Clarity, conciseness, and fluency
  * Consistent and formal terminology throughout

* Avoid:
  * Redundancy, cliches, and unnecessary embellishments
  * Overly terse, overly literal, or informal phrasing

* Use clear, direct, and professional English with formal tone
* Keep the message readable and precise - brief but not overly minimal
* Output only the result - do not add any introductory or closing remarks

For context, here are the last ${context_messages} commit messages:

\`\`\`
EOF

  # Append the last ${context_messages} commit messages
  # Filter out "Signed-off-by" lines and remove excessive blank lines
  # Here, `sed` is used to collapse multiple consecutive blank lines into a single blank line
  git log -n "${context_messages}" --pretty=format:"%B" | grep -v "^Signed-off-by:" | sed '/^$/N;/^\n$/D'

  cat <<'EOF'
```

Below is the diff showing the changes that are to be included in the commit itself:

```
EOF

  # Append the diff of staged changes to the prompt
  if is_any_argument short "$@"; then
    # Use a minimal diff if 'short' argument is provided
    git diff --diff-algorithm=minimal --no-color --cached
    echo "A short prompt was generated." >&2
  else
    # Otherwise, use a more detailed histogram diff with function context
    git diff --diff-algorithm=histogram --no-color --cached --function-context -U15
  fi

  # Close the commit messages section
  echo '```'
}

# ## `display_help`
#
# Displays the help message for the script.
#
# This function prints usage instructions, a description of the script,
# and details about available commands. It determines the script's name
# dynamically for use in the help text.
#
# ### Usage
#
# display_help
#
display_help() {
  # Get the script's base name for the usage instructions
  local script_name; script_name="$(basename "$0")"

  cat <<EOF
Usage: ${script_name:-"commit-prompt"} [COMMAND...]

Generates a commit message suggestion based on staged Git changes by
querying the Large Language Model (LLM).

The suggested message is then used to prepare a Git commit.

Commands:
  short          Generates a more concise prompt. This involves using a minimal
                 diff of staged changes, which can be helpful if the default
                 prompt is too long for the AI model's input limits.

  copy           Generates the prompt and copies it to the system clipboard,
                 then exits. This option bypasses querying the AI model,
                 allowing you to use the prompt elsewhere or review it manually.

  help, -h, --help
                 Displays this help message and exits.

Behavior:
- If no commands are given, the script generates a full prompt, queries the
  LLM, and then opens 'git commit' with the AI-suggested message.
- The commands 'short' and 'copy' can be specified in any order.

Requirements:
- Git must be installed and used in a Git repository.
- The GEMINI_API_KEY environment variable must be set to query the Gemini.
EOF
}

# Main script starts here

# Check for help argument first; this takes precedence
if is_any_argument "help" "$@" || is_any_argument "--help" "$@" || is_any_argument "-h" "$@"; then
  display_help
  exit 0
fi

# Check if there are any changes staged for commit
if git diff --cached --quiet; then
  echo "No changes have been staged for commit." >&2
  exit 1
fi

# Generate the prompt using staged changes and commit history
# Pass all script arguments to make_prompt for 'short' keyword detection
prompt="$(make_prompt "$@")"

# Check if the generated prompt exceeds ChatGPT's web interface length limit.
# The script continues even if the prompt is too long, allowing clipboard copy
if (( "${#prompt}" > chatgpt_limit )); then
  if is_any_argument short "$@"; then
    # Message displayed if the 'short' argument is used but the prompt is still too long
    echo "The prompt remains too long for ChatGPT's limit, even with the 'short' option." >&2
  else
    # Suggest using the 'short' argument if the prompt is too long
    echo "The prompt exceeds ChatGPT's length limit. To reduce its size, consider running:" >&2
    echo "  $(basename "$0") short" >&2
  fi
fi

# Copy the generated prompt to the system clipboard
<<<"${prompt}" copy_to_clipboard

# If 'copy' argument is present, skip querying the language model and exit
if is_any_argument copy "$@"; then
  echo "The 'copy' argument was provided; skipping query to the language model." >&2
  exit
fi

# Initialize the success flag to false - this flag will be used to track if 
# at least one language model query was successful during the loop iteration.
success=false

# Loop through the array with a step of 2
for ((i = 0; i < ${#models[@]}; i += 2)); do
  model_type="${models[i]}"
  model_name="${models[i + 1]}"

  # Attempt to query the language model with the specified type and name
  if response="$(query_language_model "${model_type}" "${model_name}" <<<"${prompt}")"; then
    success=true
    break
  fi
done

# Check if no model succeeded based on the exit status
if [ "${success}" = false ]; then
  echo "Failed to retrieve a response from all specified language models." >&2
  exit 1
fi

# Use the response from the language model as a template for the git commit message
echo "Making a commit with the response as the template..." >&2
git commit --template <(echo "${response}")
